{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "This notebook demonstrates defect detection on a set of chip wafer maps.\n",
    "\n",
    "## Data Source\n",
    "\n",
    "[Qingyi](https://www.kaggle.com/qingyi). (February 2018). WM-811K wafer map, Version 1. Retrieved January 2018 from https://www.kaggle.com/qingyi/wm811k-wafer-map/downloads/wm811k-wafer-map.zip/1.\n",
    "    \n",
    "### References\n",
    "\n",
    "* See also this [kernel](https://www.kaggle.com/ashishpatel26/wm-811k-wafermap) which has graphs showing class distribution.\n",
    "* This [script](https://github.com/caslabai/wafer-inspection/blob/master/dataset/pkl2tfrecord.py) has additional data loading code.\n",
    "\n",
    "Copyright 2019 Amazon.com, Inc. or its affiliates. All Rights Reserved.\n",
    "SPDX-License-Identifier: MIT-0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load raw data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_pickle('raw-data/LSWMD.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Explore raw data\n",
    "\n",
    "## Dimensions\n",
    "\n",
    "Let's look at the range in image dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_dim(x):\n",
    "    dim0=np.size(x,axis=0)\n",
    "    dim1=np.size(x,axis=1)\n",
    "    return dim0,dim1\n",
    "dataset['waferMapDim']=dataset.waferMap.apply(find_dim)\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dataset.waferMapDim), min(dataset.waferMapDim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** The dimensions are all over the map, so we'll have to normalize them as a transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pixel values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_pixel_min_max(x):\n",
    "    dim0=np.min(x)\n",
    "    dim1=np.max(x)\n",
    "    return dim0,dim1\n",
    "dataset['pixelRange']=dataset.waferMap.apply(find_pixel_min_max)\n",
    "dataset.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max(dataset.pixelRange), min(dataset.pixelRange)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset['failureNum']=dataset.failureType\n",
    "dataset['trainTestNum']=dataset.trianTestLabel\n",
    "mapping_type={'Center':0,'Donut':1,'Edge-Loc':2,'Edge-Ring':3,'Loc':4,'Random':5,'Scratch':6,'Near-full':7,'none':8}\n",
    "mapping_traintest={'Training':0,'Test':1}\n",
    "dataset=dataset.replace({'failureNum':mapping_type, 'trainTestNum':mapping_traintest})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_withlabel = dataset[(dataset['failureNum']>=0) & (dataset['failureNum']<=8)]\n",
    "df_withlabel =df_withlabel.reset_index()\n",
    "df_withpattern = dataset[(dataset['failureNum']>=0) & (dataset['failureNum']<=7)]\n",
    "df_withpattern = df_withpattern.reset_index()\n",
    "df_nonpattern = dataset[(dataset['failureNum']==8)]\n",
    "df_withlabel.shape[0], df_withpattern.shape[0], df_nonpattern.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import gridspec\n",
    "tol_wafers = dataset.shape[0]\n",
    "fig = plt.figure(figsize=(20, 4.5)) \n",
    "gs = gridspec.GridSpec(1, 2, width_ratios=[1, 2.5]) \n",
    "ax1 = plt.subplot(gs[0])\n",
    "ax2 = plt.subplot(gs[1])\n",
    "\n",
    "no_wafers=[tol_wafers-df_withlabel.shape[0], df_withpattern.shape[0], df_nonpattern.shape[0]]\n",
    "\n",
    "colors = ['silver', 'blue', 'green']\n",
    "explode = (0.1, 0, 0)  # explode 1st slice\n",
    "labels = ['no-label','label&pattern','label&non-pattern']\n",
    "ax1.pie(no_wafers, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', shadow=True, startangle=140)\n",
    "\n",
    "uni_pattern=np.unique(df_withpattern.failureNum, return_counts=True)\n",
    "labels2 = ['','Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']\n",
    "ax2.bar(uni_pattern[0],uni_pattern[1]/df_withpattern.shape[0], color='blue', align='center', alpha=0.9)\n",
    "ax2.set_title(\"failure type frequency\")\n",
    "ax2.set_ylabel(\"% of pattern wafers\")\n",
    "ax2.set_xticklabels(labels2)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uni_set=np.unique(df_withlabel.trainTestNum, return_counts=True)\n",
    "N = len(uni_set[0])\n",
    "ind = np.arange(N)\n",
    "width = 0.35       \n",
    "labels3 = ['Training','Test']\n",
    "p1 = plt.bar(ind, uni_set[1]/df_withlabel.shape[0]*100, width)\n",
    "\n",
    "plt.ylabel('% of train/test')\n",
    "plt.title('split between train/test')\n",
    "plt.xticks(ind, labels3)\n",
    "plt.yticks(np.arange(0, 101, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conclusion** The distributions are not even, so we'll want to take care to write them equally between training, test, and validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize images in each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = [0,1,2,3,4,5,6,7]\n",
    "labels2 = ['Center','Donut','Edge-Loc','Edge-Ring','Loc','Random','Scratch','Near-full']\n",
    "\n",
    "for k in x:\n",
    "    fig, ax = plt.subplots(nrows = 1, ncols = 10, figsize=(18, 12))\n",
    "    ax = ax.ravel(order='C')\n",
    "    for j in [k]:\n",
    "        img = df_withpattern.waferMap[df_withpattern.failureType==labels2[j]]\n",
    "        for i in range(10):\n",
    "            ax[i].imshow(img[img.index[i]])\n",
    "            ax[i].set_title(df_withpattern.failureType[img.index[i]][0][0], fontsize=10)\n",
    "            ax[i].set_xlabel(df_withpattern.index[img.index[i]], fontsize=10)\n",
    "            ax[i].set_xticks([])\n",
    "            ax[i].set_yticks([])\n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write images to disk\n",
    "\n",
    "We'll use the standard layout where we have one folder per data set \n",
    "(train/test), and inside each of those we have one folder per label.\n",
    "\n",
    "We'll use a stratified split so we maintain the ratio between classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "import math\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "DATA = Path('vdata')\n",
    "scale_factor = math.floor(255.0 / 2.0)\n",
    "\n",
    "images      =   df_withlabel[\"waferMap\"]\n",
    "labels      =   df_withlabel[\"failureType\"].apply(str)\n",
    "\n",
    "img_train, img_test, label_train, label_test = train_test_split(images, \n",
    "                 labels,\n",
    "                test_size=0.2,\n",
    "                stratify=labels)\n",
    "img_train_v, img_valid, label_train_v, label_valid = train_test_split(img_train, \n",
    "                 label_train,\n",
    "                test_size=0.2,\n",
    "                stratify=label_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(img_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def writeImgToDisk(imgdata, labeldata, dset, scale, parent):\n",
    "    cnt = 0\n",
    "    pset = Path(parent/dset)\n",
    "    for img, label in zip(imgdata, labeldata):\n",
    "\n",
    "        dclass = label[3:-3]\n",
    "        ipath = Path(pset/dclass)\n",
    "\n",
    "        if ipath.exists() == False:\n",
    "            ipath.mkdir(parents=True)\n",
    "            print(\"Making \" + str(ipath))\n",
    "\n",
    "        img_scaled = img * scale\n",
    "\n",
    "        fname = str(cnt) + '.png'\n",
    "        imageio.imwrite(uri=Path(ipath/fname), im=img_scaled, format='PNG-PIL')\n",
    "        cnt = cnt + 1\n",
    "\n",
    "    print(\"Wrote {0} images\".format(str(cnt)))\n",
    "    for child in pset.iterdir(): \n",
    "        if child.is_dir():\n",
    "            child_cnt = len([x for x in child.iterdir() if x.is_file()])\n",
    "            print(\"For class \" + child.stem + \", wrote \" + str(child_cnt) + \" images\")\n",
    "                    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeImgToDisk(img_train_v, label_train_v, 'train', scale_factor, DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeImgToDisk(img_test, label_test, 'test', scale_factor, DATA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writeImgToDisk(img_valid, label_valid, 'valid', scale_factor, DATA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize\n",
    "\n",
    "Let's load up our saved images for a spot check."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision import datasets\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data.sampler import RandomSampler\n",
    "\n",
    "# helper function to un-normalize and display an image\n",
    "def imshow(img):\n",
    "    plt.imshow(np.transpose(img, (1, 2, 0)))  # convert from Tensor image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of subprocesses to use for data loading\n",
    "num_workers = 0\n",
    "# how many samples per batch to load\n",
    "batch_size = 20\n",
    "# percentage of training set to use as validation\n",
    "valid_size = 0.2\n",
    "\n",
    "train_transforms = transforms.Compose([transforms.Resize((32,32)),\n",
    "                                      transforms.ToTensor()]) \n",
    "\n",
    "train_data = datasets.ImageFolder(DATA/'train', transform=train_transforms)\n",
    "random_sampler = RandomSampler(train_data)\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, num_workers=num_workers,\n",
    "                                          sampler=random_sampler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# obtain one batch of training images\n",
    "dataiter = iter(train_loader)\n",
    "images, labels = dataiter.next()\n",
    "images = images.numpy() # convert images to numpy for display\n",
    "\n",
    "# plot the images in the batch, along with the corresponding labels\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "# display 20 images\n",
    "for idx in np.arange(20):\n",
    "    ax = fig.add_subplot(2, 20/2, idx+1, xticks=[], yticks=[])\n",
    "    imshow(images[idx])\n",
    "    ax.set_title(train_data.classes[labels[idx]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
